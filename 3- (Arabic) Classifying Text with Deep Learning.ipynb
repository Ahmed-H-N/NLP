{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- (Arabic) Recipe 6-2. Classifying Text with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP pipeline will remain the same as done in earlier notebooks. The only change would be that instead of using machine learning algorithms, we would be building models using deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"arabic_dataset_classifiction.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset source : [DataSet for Arabic Classification](https://data.mendeley.com/datasets/v524p5dhpj/2) <br>\n",
    "it is collected from 3 Arabic online newspapers: Assabah, Hespress and Akhbarona using semi-automatic web crawling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>targe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تزال صناعة الجلود في المغرب تتبع الطريقة التقل...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  targe\n",
       "0  بين أستوديوهات ورزازات وصحراء مرزوكة وآثار ولي...      0\n",
       "1  قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر ع...      0\n",
       "2  أخبارنا المغربية الوزاني تصوير الشملالي ألهب ا...      0\n",
       "3  اخبارنا المغربية قال ابراهيم الراشدي محامي سعد...      0\n",
       "4  تزال صناعة الجلود في المغرب تتبع الطريقة التقل...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.rename(columns={\"targe\": \"Target\", \"text\": \"Text\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111728, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text      object\n",
       "Target     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'قالت إنها انتظرت خمس سنوات لتشارك في فد تي في قالت الفنانة بديعة الصنهاجي التي تتقمص دور عبلة في سيتكوم ديما جيران على شاشة القناة الثانية إن قلة أعمالها التلفزيونية راجعة إلى عدم تفكيرها في الاحتراف بعد تخرجها من معهد الفن المسرحي والتنشيط الثقافي وأضافت بديعة في تصريح الصباح أن دراستها المسرح لمدة سبع سنوات كانت من باب الهواية مشيرة إلى أنها ركزت أثناء دراستها على تجسيد أدوار باللغة الفرنسية لمسرحيات كلاسيكية لموليير وتشيكوف وكان أول ظهور لبديعة الصنهاجي على شاشة التلفزيون رفقة المخرج نور الدين لخماري في السلسلة البوليسية القضية »، التي تولى بعض أصدقائها مهمة اقتراحها للعمل فيها وأوضحت الصنهاجي بخصوص دورها في سلسلة القضية أنه من الشخصيات التي تعتز بتقمصها رغم أنه من الأدوار الثانوية مضيفة أن بحكم وظيفتها لم يكن ممكنا بالنسبة إليها الغياب لمدة طويلة عن العمل إن احتراف الفن أمر صعب وهذا ما جعلني بعيدة لعدة سنوات عنه كما أن ارتباطي بعملي في مجالي التسويق والاتصال في مجال الوكالات العقارية ساهم في ذلك »، تقول بديعة عن أسباب تأخرها في دخول المجال الفني وأكدت بديعة أنها من المعجبات بأداء الفنان حسن الفد فبعد عرض السلسلة الفكاهية شانيلي تي في سنة بحثت عن رقمه الهاتفي وطلبت منه باعتبارها طالبة بالمعهد المسرحي أن يسند إليها أحد الأدوار في عمل جديد وانتظرت الفنانة بديعة الصنهاجي خمس سنوات بعد تلقيها وعدا من الفنان حسن الفد على العمل رفقته مؤكدة في هذا الصدد أنه لم يكن مهما بالنسبة إليها العمل من أجل الظهور والانتشار أو البحث عن ربح مالي بقدر ما كان تقديم عمل مميز ووصفت بديعة تجربتها مع الفنان حسن الفد في سلسلة فد تي في بالمهمة إذ قدمت ضمنها أدوارا كان من الممكن أن تؤديها على مدار عشر سنوات إذ في كل حلقة كانت تتقمص شخصيتين أو ثلاث شخصيات ووجدت الفنانة بديعة الصنهاجي نفسها أمام اختيار صعب حين عرض عليها العمل في مسلسل زينة الحياة »، الذي يعرض حاليا على شاشة القناة الأولى »، لكنها حسمت الأمر واختارت التخلي عن وظيفتها والتفرغ بشكل نهائي لمجال التمثيل خاصة أن تصوير دورها استغرق أربعة أشهر وعن دورها في سيتكوم ديما جيران »، تقول إن عبلة من الشخصيات الفكاهية التي لم تكن تتوانى أثناء تقمصها في أخذ رأي الطاقم الفني والتقني للعمل حول أدائها لها وملاحظاتهم حولها مضيفة أنها دائما كان ينتابها شك وهو أمر إيجابي بالنسبة إلى أي فنان على حد قولها يرغب في تطوير مستواه وتقديم الأفضل أمينة كندي'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data[Data['Target'] == 0].Text.iloc[350]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by observing the content, i realized that the labels are represntative of the following <br>\n",
    "- 0 celebrities <br>\n",
    "- 1 crimes <br>\n",
    "- 2 economy <br>\n",
    "- 3 politics <br>\n",
    "- 4 sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting non null data\n",
    "Data = Data[pd.notnull(Data['Text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108789, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "4    43675\n",
       "3    20485\n",
       "1    16728\n",
       "2    14165\n",
       "0    13736\n",
       "Name: Text, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.groupby('Target').Text.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use sports-crimes classification as a start, then i will include the remaining classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do the clustering for just 200 documents. Its easier to interpret.\n",
    "# i will take 200/5 = 40 samples from each class \n",
    "\n",
    "sample_crime = Data[Data['Target'] == 1].sample(n=16000)\n",
    "sample_sports = Data[Data['Target'] == 4].sample(n=16000)\n",
    "\n",
    "# Here we recreate a 'balanced' dataset.\n",
    "Data_sample = pd.concat([sample_crime, sample_sports],axis=0)\n",
    "Data_sample.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_sample['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset the target values to 0 and 1 insted of 1 and 4\n",
    "Data_sample['Target'].replace(1, 0, inplace=True)\n",
    "Data_sample['Target'].replace(4, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "              0 : 'crimes', \n",
    "              1 : 'sports'\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('arabic')\n",
    "\n",
    "Data_sample['Text'] = Data_sample['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and multiple spaces\n",
    "\n",
    "import re\n",
    "\n",
    "pattern_punctuation = '[^a-zA-z0-9ء-ي\\s]' # for punctuation (not numeric nor arabic nor english letters)\n",
    "pattern_multi_spaces = '[ ]{2,}'\n",
    "\n",
    "Data_sample['Text'] = Data_sample['Text'].apply(lambda x: re.sub(pattern_punctuation, '' , x))\n",
    "Data_sample['Text'] = Data_sample['Text'].apply(lambda x: re.sub(pattern_multi_spaces, ' ' , x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming \n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Load 'stemmer'\n",
    "stemmer = SnowballStemmer(\"arabic\")\n",
    "Data_sample['Text'] = Data_sample['Text'].apply(lambda sentence: ' '.join([stemmer.stem(word) for word in sentence.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49664 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Train and test split with 80:20 ratio\n",
    "train, test = train_test_split(Data_sample, test_size=0.2)\n",
    "\n",
    "# Define the sequence lengths, max number of words and embedding dimensions\n",
    "# Sequence length of each sentence. If more, truncate. If less, pad with zeros\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "# Top 20000 frequently occurring words\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# Get the frequently occurring words\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train.Text)\n",
    "\n",
    "# tokenizing while keeping only the top frequent words\n",
    "train_sequences = tokenizer.texts_to_sequences(train.Text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Text)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at : https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4991,\n",
       " 104,\n",
       " 164,\n",
       " 43,\n",
       " 15,\n",
       " 8,\n",
       " 679,\n",
       " 371,\n",
       " 996,\n",
       " 241,\n",
       " 2062,\n",
       " 14,\n",
       " 1308,\n",
       " 101,\n",
       " 12499,\n",
       " 214,\n",
       " 22,\n",
       " 455,\n",
       " 2593,\n",
       " 569,\n",
       " 54,\n",
       " 759,\n",
       " 661,\n",
       " 2311,\n",
       " 824,\n",
       " 911,\n",
       " 147,\n",
       " 2856,\n",
       " 1291,\n",
       " 371,\n",
       " 537,\n",
       " 58,\n",
       " 5915,\n",
       " 28,\n",
       " 5262,\n",
       " 11,\n",
       " 55,\n",
       " 7901,\n",
       " 661,\n",
       " 241,\n",
       " 6507,\n",
       " 2505,\n",
       " 24,\n",
       " 11,\n",
       " 241,\n",
       " 172,\n",
       " 2593,\n",
       " 55,\n",
       " 54,\n",
       " 212,\n",
       " 107,\n",
       " 66,\n",
       " 88,\n",
       " 2237,\n",
       " 1154,\n",
       " 54,\n",
       " 12203,\n",
       " 477,\n",
       " 32,\n",
       " 270,\n",
       " 121,\n",
       " 3668,\n",
       " 3003,\n",
       " 1098,\n",
       " 9,\n",
       " 55,\n",
       " 581,\n",
       " 187,\n",
       " 43,\n",
       " 15,\n",
       " 8,\n",
       " 371,\n",
       " 2442,\n",
       " 3166,\n",
       " 3836,\n",
       " 312,\n",
       " 807,\n",
       " 661,\n",
       " 241,\n",
       " 1205,\n",
       " 1819,\n",
       " 24,\n",
       " 11,\n",
       " 679,\n",
       " 11,\n",
       " 581,\n",
       " 187,\n",
       " 394,\n",
       " 152,\n",
       " 242,\n",
       " 232,\n",
       " 194,\n",
       " 996,\n",
       " 679,\n",
       " 60]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at the tokenization done by keras (which splits words and gives each unique word to a unique integer)\n",
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95   129   175   386   322   208   "
     ]
    }
   ],
   "source": [
    "# look at the lengths of the first 6 documents\n",
    "for i in range(6):\n",
    "    print(len(train_sequences[i]), ' ', end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600, 300)\n",
      "(6400, 300)\n"
     ]
    }
   ],
   "source": [
    "# pad train and test sequences\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,  4991,   104,\n",
       "         164,    43,    15,     8,   679,   371,   996,   241,  2062,\n",
       "          14,  1308,   101, 12499,   214,    22,   455,  2593,   569,\n",
       "          54,   759,   661,  2311,   824,   911,   147,  2856,  1291,\n",
       "         371,   537,    58,  5915,    28,  5262,    11,    55,  7901,\n",
       "         661,   241,  6507,  2505,    24,    11,   241,   172,  2593,\n",
       "          55,    54,   212,   107,    66,    88,  2237,  1154,    54,\n",
       "       12203,   477,    32,   270,   121,  3668,  3003,  1098,     9,\n",
       "          55,   581,   187,    43,    15,     8,   371,  2442,  3166,\n",
       "        3836,   312,   807,   661,   241,  1205,  1819,    24,    11,\n",
       "         679,    11,   581,   187,   394,   152,   242,   232,   194,\n",
       "         996,   679,    60])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the padding is done at the beginning, why ?\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300   300   300   300   300   300   "
     ]
    }
   ],
   "source": [
    "# look at the lengths of the first 6 documents\n",
    "for i in range(6):\n",
    "    print(len(train_data[i]), ' ', end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22058    1\n",
       "23432    1\n",
       "4750     0\n",
       "1046     0\n",
       "323      0\n",
       "15067    0\n",
       "28157    1\n",
       "24026    1\n",
       "27963    1\n",
       "13669    0\n",
       "31232    1\n",
       "13881    0\n",
       "7572     0\n",
       "21291    1\n",
       "23258    1\n",
       "17153    1\n",
       "26471    1\n",
       "1030     0\n",
       "17600    1\n",
       "13900    0\n",
       "22479    1\n",
       "106      0\n",
       "5053     0\n",
       "19323    1\n",
       "18590    1\n",
       "11117    0\n",
       "12738    0\n",
       "21682    1\n",
       "19296    1\n",
       "123      0\n",
       "        ..\n",
       "5839     0\n",
       "2867     0\n",
       "29763    1\n",
       "16332    1\n",
       "15633    0\n",
       "11325    0\n",
       "7348     0\n",
       "16611    1\n",
       "23254    1\n",
       "20544    1\n",
       "24562    1\n",
       "6556     0\n",
       "8101     0\n",
       "28105    1\n",
       "5384     0\n",
       "11187    0\n",
       "11772    0\n",
       "4964     0\n",
       "16218    1\n",
       "26072    1\n",
       "24993    1\n",
       "4403     0\n",
       "6631     0\n",
       "10565    0\n",
       "23662    1\n",
       "10548    0\n",
       "8567     0\n",
       "8211     0\n",
       "15432    0\n",
       "20611    1\n",
       "Name: Target, Length: 25600, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25600, 300)\n",
      "Shape of label tensor: (25600, 2)\n",
      "Shape of label tensor: (6400, 2)\n"
     ]
    }
   ],
   "source": [
    "# changing labels to one-hot-encoding \n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building and predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building the models using different deep learning approaches\n",
    "like CNN, RNN, LSTM, and Bidirectional LSTM and comparing the\n",
    "performance of each model using different accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the CNN model.\n",
    "\n",
    "Here we define a single hidden layer with 128 memory units. The\n",
    "network uses a dropout with a probability of 0.5. The output layer is a\n",
    "dense layer using the softmax activation function to output a probability\n",
    "prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN, Bidirectional, LSTM\n",
    "from keras.layers import BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note in this notebook**: for feature engineering, we will use word embeddings as features . the embeddings was not pretraind, however it was learned from the inputed data during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN 1D model.\n"
     ]
    }
   ],
   "source": [
    "print('Training CNN 1D model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = MAX_NB_WORDS, output_dim = EMBEDDING_DIM, input_length = MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now fitting our model to the data. Here we have 5 epochs and a\n",
    "batch size of 64 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/5\n",
      "25600/25600 [==============================] - 86s 3ms/step - loss: 0.1123 - acc: 0.9534 - val_loss: 0.2490 - val_acc: 0.9136\n",
      "Epoch 2/5\n",
      "25600/25600 [==============================] - 83s 3ms/step - loss: 0.0289 - acc: 0.9938 - val_loss: 0.0416 - val_acc: 0.9925\n",
      "Epoch 3/5\n",
      "25600/25600 [==============================] - 90s 4ms/step - loss: 0.0215 - acc: 0.9955 - val_loss: 0.0588 - val_acc: 0.9937\n",
      "Epoch 4/5\n",
      "25600/25600 [==============================] - 101s 4ms/step - loss: 0.0218 - acc: 0.9957 - val_loss: 0.0648 - val_acc: 0.9906\n",
      "Epoch 5/5\n",
      "25600/25600 [==============================] - 101s 4ms/step - loss: 0.0170 - acc: 0.9968 - val_loss: 0.0806 - val_acc: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xa14569400>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train, batch_size=64, epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2207178e-19 1.0000000e+00]\n",
      "[6.254029e-05 9.999374e-01]\n",
      "[0.49948114 0.50051886]\n",
      "[9.99988079e-01 1.18791995e-05]\n",
      "[3.0698534e-11 1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# note that the `prediction` variable is the predicted probability for each class (so each row sum up to 1)\n",
    "# let's see the first 5 predictions \n",
    "for i in range(5):\n",
    "    print(predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.9965035  0.98432698]\n",
      "recall: [0.98399247 0.99657747]\n",
      "fscore: [0.99020846 0.99041435]\n",
      "support: [3186 3214]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99      3186\n",
      "          1       0.98      1.00      0.99      3214\n",
      "\n",
      "avg / total       0.99      0.99      0.99      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test using a custom input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_test = 'سيقام، كأس العالم فى قطر عام 2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go trhough the prerpcessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "processed_text = \" \".join(x for x in text_to_test.split() if x not in stop)\n",
    "\n",
    "# remove punctuation and multiple spaces\n",
    "processed_text = re.sub(pattern_punctuation, '' , processed_text)\n",
    "processed_text = re.sub(pattern_multi_spaces, ' ' , processed_text)\n",
    "\n",
    "# stemming \n",
    "processed_text = ' '.join([stemmer.stem(word) for word in processed_text.split()])\n",
    "\n",
    "# tokenizing\n",
    "text_series_to_test = pd.Series(processed_text)\n",
    "text_series_to_test =  tokenizer.texts_to_sequences(text_series_to_test)\n",
    "\n",
    "# padding\n",
    "text_series_to_test = pad_sequences(text_series_to_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02185888, 0.9781411 ]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.8% class sport (correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SimpleRNN model.\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "print('Training SimpleRNN model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/5\n",
      "25600/25600 [==============================] - 128s 5ms/step - loss: 0.1603 - accuracy: 0.9786 - val_loss: 0.0810 - val_accuracy: 0.9841\n",
      "Epoch 2/5\n",
      "25600/25600 [==============================] - 127s 5ms/step - loss: 0.0577 - accuracy: 0.9880 - val_loss: 0.0744 - val_accuracy: 0.9809\n",
      "Epoch 3/5\n",
      "25600/25600 [==============================] - 127s 5ms/step - loss: 0.0430 - accuracy: 0.9902 - val_loss: 0.0719 - val_accuracy: 0.9808\n",
      "Epoch 4/5\n",
      "25600/25600 [==============================] - 145s 6ms/step - loss: 0.0290 - accuracy: 0.9939 - val_loss: 0.0679 - val_accuracy: 0.9839\n",
      "Epoch 5/5\n",
      "25600/25600 [==============================] - 162s 6ms/step - loss: 0.0234 - accuracy: 0.9952 - val_loss: 0.0717 - val_accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xa18a6a6a0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics = ['accuracy'])\n",
    "model.fit(train_data, labels_train, batch_size=16, epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00379755, 0.9962024 ],\n",
       "       [0.01539066, 0.98460937],\n",
       "       [0.9919704 , 0.00802953],\n",
       "       ...,\n",
       "       [0.00157173, 0.99842834],\n",
       "       [0.00646456, 0.9935354 ],\n",
       "       [0.002992  , 0.997008  ]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.9817782  0.98103823]\n",
      "recall: [0.98085374 0.98195395]\n",
      "fscore: [0.98131575 0.98149588]\n",
      "support: [3186 3214]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      3186\n",
      "          1       0.98      0.98      0.98      3214\n",
      "\n",
      "avg / total       0.98      0.98      0.98      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test using a custom input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9380894 , 0.06191062]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93.8% class crime (wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model.\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "print('Training LSTM model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"relu\", return_sequences=True, units=16, recurrent_activation=\"hard_sigmoid\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/5\n",
      "25600/25600 [==============================] - 238s 9ms/step - loss: 0.0532 - accuracy: 0.9823 - val_loss: 0.0343 - val_accuracy: 0.9928\n",
      "Epoch 2/5\n",
      "25600/25600 [==============================] - 234s 9ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.0437 - val_accuracy: 0.9925\n",
      "Epoch 3/5\n",
      "25600/25600 [==============================] - 259s 10ms/step - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.0509 - val_accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "25600/25600 [==============================] - 258s 10ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.0551 - val_accuracy: 0.9919\n",
      "Epoch 5/5\n",
      "25600/25600 [==============================] - 298s 12ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0589 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xa1c3dfdd8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.fit(train_data, labels_train, batch_size=16, epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4037952e-16, 1.0000000e+00],\n",
       "       [2.7975549e-11, 1.0000000e+00],\n",
       "       [9.9799418e-01, 2.0058097e-03],\n",
       "       ...,\n",
       "       [3.4818143e-10, 1.0000000e+00],\n",
       "       [8.1087570e-16, 1.0000000e+00],\n",
       "       [2.8518691e-13, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.9881583  0.99529928]\n",
      "recall: [0.9952919  0.98817673]\n",
      "fscore: [0.99171228 0.99172521]\n",
      "support: [3186 3214]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      3186\n",
      "          1       1.00      0.99      0.99      3214\n",
      "\n",
      "avg / total       0.99      0.99      0.99      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,\n",
    "predicted_lstm.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test using a custom input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4892155e-04, 9.9955100e-01]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99.9% class sport (correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Bidirectional LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, LSTM preserves information from inputs using the\n",
    "hidden state. In bidirectional LSTMs, inputs are fed in two ways: one\n",
    "from previous to future and the other going backward from future to\n",
    "past, helping in learning future representation as well. Bidirectional\n",
    "LSTMs are known for producing very good results as they are capable of\n",
    "understanding the context better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bidirectional LSTM model.\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "print('Training Bidirectional LSTM model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/3\n",
      "25600/25600 [==============================] - 279s 11ms/step - loss: 0.0398 - accuracy: 0.9862 - val_loss: 0.0227 - val_accuracy: 0.9936\n",
      "Epoch 2/3\n",
      "25600/25600 [==============================] - 288s 11ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0306 - val_accuracy: 0.9927\n",
      "Epoch 3/3\n",
      "25600/25600 [==============================] - 287s 11ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0333 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xa251868d0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.fit(train_data, labels_train, batch_size=16, epochs=3, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3783587e-07, 9.9999964e-01],\n",
       "       [9.6426636e-05, 9.9990356e-01],\n",
       "       [9.9645293e-01, 3.5470494e-03],\n",
       "       ...,\n",
       "       [7.2406688e-06, 9.9999273e-01],\n",
       "       [6.1590015e-07, 9.9999940e-01],\n",
       "       [4.0449393e-07, 9.9999964e-01]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_blstm=model.predict(test_data)\n",
    "predicted_blstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.99557102 0.98795925]\n",
      "recall: [0.98775895 0.99564406]\n",
      "fscore: [0.9916496  0.99178677]\n",
      "support: [3186 3214]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99      3186\n",
      "          1       0.99      1.00      0.99      3214\n",
      "\n",
      "avg / total       0.99      0.99      0.99      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test,\n",
    "predicted_blstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_blstm.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test using a custom input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.705105e-06, 9.999913e-01]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(text_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99.9% class sport (correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that deep learning models give much higher accuracy than the classical models. Bidirectional LSTM gives the highest accuracy among the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
